# -*- coding: utf-8 -*-
"""200411100199-UTS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DsKPp--ZYTmyLj5t_lcZM0x0HMRrkX6R
"""

# from google.colab import drive
# drive.mount('/content/drive')

# %cd /content/drive/MyDrive/prosaindata/

# !pip install nest-asyncio
# !git clone --depth=1 https://github.com/twintproject/twint.git
# %cd twint
# !pip3 install . -r requirements.txt

# !pip install aiohttp==3.7.0

# import nest_asyncio
# nest_asyncio.apply() #digunakan sekali untuk mengaktifkan tindakan serentak dalam notebook jupyter.
# import twint #untuk import twint
# c = twint.Config()
# c.Search = 'prabowo'
# c.Pandas = True
# c.Limit = 70
# twint.run.Search(c)
# Tweets_dfs = twint.storage.panda.Tweets_df
# Tweets_dfs["tweet"]

"""# **Mengambil Data**

Proses ini digunakan untuk data darigithub "dataset-baru" abstrak yang telah disimpan dalam github dengan format .csv


"""

#install library pandas
!pip install pandas

#install library numpy
!pip install numpy

import pandas as pd 
import numpy as np

data_abstrak = pd.read_csv("https://raw.githubusercontent.com/smithtarger/webmining/main/dataset-baru.csv")

data_abstrak

#install library sastrawi
!pip install sastrawi

#install library swifter
!pip install swifter

"""# **Case Folding**

Tahap untuk merubah teks yang memiliki huruf kapital menjadi huruf kecil
"""

data_abstrak['Abstrak'] = data_abstrak['Abstrak'].str.lower()


data_abstrak['Abstrak']

#install library nltk
!pip install nltk

"""## Menghapus Karakter Spesial"""

import string 
import re #regex library
# import word_tokenize & FreqDist from NLTK

from nltk.tokenize import word_tokenize 
from nltk.probability import FreqDist

# ------ Tokenizing ---------

def remove_special(text):
    # remove tab, new line, ans back slice
    text = text.replace('\\t'," ").replace('\\n'," ").replace('\\u'," ").replace('\\'," ").replace('\\f'," ").replace('\\r'," ")
    # remove non ASCII (emoticon, chinese word, .etc)
    text = text.encode('ascii', 'replace').decode('ascii')
    # remove mention, link, hashtag
    text = ' '.join(re.sub("([@#][A-Za-z0-9]+)|(\w+:\/\/\S+)"," ", text).split())
    # remove incomplete URL
    return text.replace("http://", " ").replace("https://", " ")
                
data_abstrak['Abstrak'] = data_abstrak['Abstrak'].apply(remove_special)
data_abstrak['Abstrak']

"""## Menghapus Angka

"""

#remove number
def remove_number(text):
    return  re.sub(r"\d+", "", text)

data_abstrak['Abstrak'] = data_abstrak['Abstrak'].apply(remove_number)
data_abstrak['Abstrak']

"""## Menghapus Tanda Baca

"""

#remove punctuation
def remove_punctuation(text):
    return text.translate(str.maketrans("","",string.punctuation))

data_abstrak['Abstrak'] = data_abstrak['Abstrak'].apply(remove_punctuation)
data_abstrak['Abstrak']

"""## Menghapus Spasi

"""

#remove whitespace leading & trailing
def remove_whitespace_LT(text):
    return text.strip()

data_abstrak['Abstrak'] = data_abstrak['Abstrak'].apply(remove_whitespace_LT)


#remove multiple whitespace into single whitespace
def remove_whitespace_multiple(text):
    return re.sub('\s+',' ',text)

data_abstrak['Abstrak'] = data_abstrak['Abstrak'].apply(remove_whitespace_multiple)
data_abstrak['Abstrak']

"""## Menghapus char Huruf 

"""

# remove single char
def remove_singl_char(text):
    return re.sub(r"\b[a-zA-Z]\b", " ", text)

data_abstrak['Abstrak'] = data_abstrak['Abstrak'].apply(remove_singl_char)
data_abstrak['Abstrak']

import nltk
nltk.download('punkt')

"""# **Tokenizing**

Tokenizing adalah proses pemisahan teks menjadi potongan-potongan yang disebut sebagai token untuk kemudian di analisa. Kata, angka, simbol, tanda baca dan entitas penting lainnya dapat dianggap sebagai token
"""

# NLTK word Tokenize 
def word_tokenize_wrapper(text):
    return word_tokenize(text)

data_abstrak['Abstrak'] = data_abstrak['Abstrak'].apply(word_tokenize_wrapper)
data_abstrak['Abstrak']

"""# **Filtering(Stopwords Removal)**
Proses untuk menghapus kata hubung atau kata yang tidak memiliki makna

"""

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')

list_stopwords = stopwords.words('indonesian')

#Menghapus Stopword dari list token
def stopwords_removal(words):
    return [word for word in words if word not in list_stopwords]

data_abstrak['Abstrak'] = data_abstrak['Abstrak'].apply(stopwords_removal)

data_abstrak['Abstrak']

"""# **Stemming**
Data training hasil dari filtering akan dilakukan pengecekan atau pencarian kata-kata yang sesuai dengan kamus umum. Apabila data training hasil filtering sesuai dengan kamus umum maka kata akan dikeluarkan sementara, karena sudah dianggap sebagai kata dasar. Apabila masih terdapat kata yang tidak termasuk dalam kata dasar maka tahap selanjutnya adalah menghapus inflection suffixes yang merupakan akhiran pertama. Kata yang memiliki akhiran partticles seperti “-pun”, “-kah”, “-tah”, “- lah” dan akhiran possessive pronoun seperti “-mu”, “-ku” dan “-nya” dihilangkan. Setelah dilakukan proses case folding, tokenezing, dan filtering, proses selanjutnya yaitu stemming. Stemming yang digunakan pada penelitian ini menggunakan algoritma Enhanced Confix Stipping Stemmer, terdiri dari beberapa langkah: Data training hasil dari filtering akan dilakukan pengecekan atau pencarian kata-kata yang sesuai dengan kamus umum. Apabila data training hasil filtering sesuai dengan kamus umum maka kata akan dikeluarkan sementara, karena sudah dianggap sebagai kata dasar. Apabila masih terdapat kata yang tidak termasuk dalam kata dasar maka tahap selanjutnya adalah menghapus inflection suffixes yang merupakan akhiran pertama. Kata yang memiliki akhiran partticles seperti “-pun”, “-kah”, “-tah”, “- lah” dan akhiran possessive pronoun seperti “-mu”, “-ku” dan “-nya” dihilangkan.
"""

from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
import swifter


# create stemmer
factory = StemmerFactory()
stemmer = factory.create_stemmer()

# stemmed
def stemmed_wrapper(term):
    return stemmer.stem(term)

term_dict = {}

for document in data_abstrak['Abstrak']:
    for term in document:
        if term not in term_dict:
            term_dict[term] = ' '
            
print(len(term_dict))
print("------------------------")

for term in term_dict:
    term_dict[term] = stemmed_wrapper(term)
    print(term,":" ,term_dict[term])
    
print(term_dict)
print("------------------------")


# apply stemmed term to dataframe
def get_stemmed_term(document):
    return [term_dict[term] for term in document]

data_abstrak['Abstrak'] = data_abstrak['Abstrak'].swifter.apply(get_stemmed_term)
data_abstrak['Abstrak']

"""##Menyimpan Hasil Tahap Preprocessing ke file .csv"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd                     
import matplotlib.pyplot as plt          # plotting
import numpy as np                       # dense matrices
from scipy.sparse import csr_matrix      # sparse matrices
# %matplotlib inline

data_abstrak.to_csv("preprocessing.csv")

"""# **TF-IDF**

TF(Term Frequency) : Istilah frekuensi kata dalam dokumen. Ada beberapa cara untuk menghitung frekuensi ini, dengan cara yang paling sederhana adalah dengan menghitung jumlah kata yang muncul dalam dokumen. Lalu, ada cara untuk menyesuaikan frekuensi, berdasarkan panjang dokumen, atau dengan frekuensi mentah kata yang paling sering muncul dalam dokumen.
"""

from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer, CountVectorizer
#Membuat Dataframe
dataTextPre = pd.read_csv('preprocessing.csv')
vectorizer = CountVectorizer(min_df=1)
bag = vectorizer.fit_transform(dataTextPre['Abstrak'])
dataTextPre

matrik_vsm=bag.toarray()
print(matrik_vsm)
matrik_vsm.shape

matrik_vsm[0]

a=vectorizer.get_feature_names_out()

print(len(matrik_vsm[:,1]))
#dfb =pd.DataFrame(data=matrik_vsm,index=df,columns=[a])
dataTF =pd.DataFrame(data=matrik_vsm,index=list(range(1, len(matrik_vsm[:,1])+1, )),columns=[a])
dataTF

from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer, CountVectorizer
DataTFIDF = TfidfVectorizer()
TFIDF = DataTFIDF.fit_transform(dataTextPre['Abstrak']).toarray()
TFIDF = pd.DataFrame(TFIDF)
TFIDF

"""# **PERCOBAAN K-MEANS

"""

from sklearn.cluster import KMeans

kmeans =KMeans(n_clusters=3)
kmeans=kmeans.fit(dataTF)
prediksi=kmeans.predict(dataTF)
centroids = kmeans.cluster_centers_

data=pd.DataFrame(prediksi,columns=["Cluster"])
data

datalabel = pd.read_csv('https://raw.githubusercontent.com/smithtarger/webmining/main/dataset-baru.csv')
dataJurnal = pd.concat([dataTF.reset_index(drop=True), datalabel["Kategori"]], axis=1)
dataJurnal

dataJurnal['Kategori'].unique()

dataJurnal.info()

"""### Split Data"""

### Train test split to avoid overfitting
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(dataJurnal.drop(labels=['Kategori'], axis=1),
    dataJurnal['Kategori'],
    test_size=0.3,
    random_state=0)

X_train

"""# KNN """

from sklearn.neighbors import KNeighborsClassifier
testing=[]
listnum=[]
for i in range(2,21):
  listnum.append(i)
  neigh = KNeighborsClassifier(n_neighbors=i)
  neigh.fit(X_train, y_train)
  Y_pred = neigh.predict(X_test) 
  testing.append(Y_pred)
testing

y_test

"""**Hasil**"""

from sklearn.metrics import make_scorer, accuracy_score,precision_score
listtest=[]
listacc=[]
for i in range(len(testing)):
  accuracy_neigh=round(accuracy_score(y_test,testing[i])* 100, 2)
  acc_neigh = round(neigh.score(X_train, y_train) * 100, 2)
  listappend=listnum[i]
  appendlist=listappend,accuracy_neigh
  listtest.append(appendlist)
  listacc.append(accuracy_neigh)
listtest

from matplotlib import pyplot as plt
plt.bar(listnum, listacc)
plt.xticks(listnum)
plt.title('Nilai Akurasi Berdasarkan Input')
plt.ylabel('Persentase Akurasi')